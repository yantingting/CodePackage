{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all' #默认为'last'\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sys.path.append('/Users/yantingting/PycharmProjects/modeling')\n",
    "sys.setrecursionlimit(100000)\n",
    "import plot_tools as pl\n",
    "import metrics\n",
    "import summary_statistics as ss\n",
    "import feature_selection as fs\n",
    "from data_io_utils import *\n",
    "import data_processing as dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_model = '/Users/yantingting/Documents/海外风控/巴西/模型初版_SDK/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23927, 77)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(14969,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3743,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5215,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>DATE</th>\n",
       "      <th>flag_result</th>\n",
       "      <th>is_sample</th>\n",
       "      <th>var17</th>\n",
       "      <th>var18</th>\n",
       "      <th>var9</th>\n",
       "      <th>age</th>\n",
       "      <th>var2-O</th>\n",
       "      <th>var3-0</th>\n",
       "      <th>...</th>\n",
       "      <th>rate_len_is9</th>\n",
       "      <th>rate_len_is12</th>\n",
       "      <th>rate_len_isover13</th>\n",
       "      <th>board_rank</th>\n",
       "      <th>app_size</th>\n",
       "      <th>electric_quantity</th>\n",
       "      <th>available_memory_size</th>\n",
       "      <th>total_memory_size</th>\n",
       "      <th>available_capacity_size</th>\n",
       "      <th>disk_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3808870852</td>\n",
       "      <td>20191014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>6.0</td>\n",
       "      <td>234</td>\n",
       "      <td>2.0</td>\n",
       "      <td>464.00</td>\n",
       "      <td>1863.68</td>\n",
       "      <td>2406.40</td>\n",
       "      <td>23930.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3808871384</td>\n",
       "      <td>20191015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>2.0</td>\n",
       "      <td>294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>367.00</td>\n",
       "      <td>1955.84</td>\n",
       "      <td>1873.92</td>\n",
       "      <td>12103.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3808917409</td>\n",
       "      <td>20191017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247967</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.260163</td>\n",
       "      <td>2.0</td>\n",
       "      <td>343</td>\n",
       "      <td>8.0</td>\n",
       "      <td>314.00</td>\n",
       "      <td>1935.36</td>\n",
       "      <td>8499.20</td>\n",
       "      <td>26030.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3808927120</td>\n",
       "      <td>20191018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>624.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6985.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049645</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>8.0</td>\n",
       "      <td>207</td>\n",
       "      <td>18.0</td>\n",
       "      <td>790.00</td>\n",
       "      <td>4003.84</td>\n",
       "      <td>29347.84</td>\n",
       "      <td>50585.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3808927373</td>\n",
       "      <td>20191018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.024155</td>\n",
       "      <td>0.551932</td>\n",
       "      <td>19.0</td>\n",
       "      <td>303</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1044.48</td>\n",
       "      <td>6051.84</td>\n",
       "      <td>76349.44</td>\n",
       "      <td>121856.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loan_id      DATE  flag_result  is_sample  var17  var18    var9   age  \\\n",
       "0  3808870852  20191014            1          1   -1.0  623.0  1032.0  54.0   \n",
       "1  3808871384  20191015            1          1   -1.0  489.0  2040.0  50.0   \n",
       "2  3808917409  20191017            1          1   -1.0  693.0  2800.0  32.0   \n",
       "3  3808927120  20191018            1          1  624.0   -1.0  6985.0  29.0   \n",
       "4  3808927373  20191018            1          2   -1.0  553.0  3000.0  37.0   \n",
       "\n",
       "   var2-O  var3-0  ...  rate_len_is9  rate_len_is12  rate_len_isover13  \\\n",
       "0       1       1  ...      0.109589       0.321918           0.287671   \n",
       "1       1       1  ...      0.431373       0.019608           0.019608   \n",
       "2       1       1  ...      0.247967       0.028455           0.260163   \n",
       "3       1       1  ...      0.049645       0.382979           0.021277   \n",
       "4       0       0  ...      0.019324       0.024155           0.551932   \n",
       "\n",
       "   board_rank  app_size  electric_quantity  available_memory_size  \\\n",
       "0         6.0       234                2.0                 464.00   \n",
       "1         2.0       294                2.0                 367.00   \n",
       "2         2.0       343                8.0                 314.00   \n",
       "3         8.0       207               18.0                 790.00   \n",
       "4        19.0       303               20.0                1044.48   \n",
       "\n",
       "   total_memory_size  available_capacity_size  disk_size  \n",
       "0            1863.68                  2406.40   23930.88  \n",
       "1            1955.84                  1873.92   12103.68  \n",
       "2            1935.36                  8499.20   26030.08  \n",
       "3            4003.84                 29347.84   50585.60  \n",
       "4            6051.84                 76349.44  121856.00  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_var = pd.read_csv(file_path_model + 'df_var.csv')\n",
    "df_var.shape\n",
    "df_train1= df_var[df_var['is_sample'] == 1]\n",
    "df_train1['flag_result'].shape\n",
    "df_train2= df_var[df_var['is_sample'] == 2]\n",
    "df_train2['flag_result'].shape\n",
    "df_test= df_var[df_var['is_sample'] == 3]\n",
    "df_test['flag_result'].shape\n",
    "df_var.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var = df_var.fillna(-1)\n",
    "X_train1 = df_train1.iloc[:, 4:]\n",
    "y_train1 = df_train1['flag_result'].astype(int)\n",
    "X_train2 = df_train2.iloc[:, 4:]\n",
    "y_train2 =df_train2['flag_result'].astype(int)\n",
    "X_test = df_test.iloc[:, 4:]\n",
    "y_test = df_test['flag_result'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstep2 训练模型，初筛变量\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.29%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "step2 训练模型，初筛变量\n",
    "'''\n",
    "model = XGBClassifier()\n",
    "eval_set = [(X_train2, y_train2)]\n",
    "model.fit( X_train1,  y_train1,  early_stopping_rounds=10,  eval_metric=\"auc\",  eval_set=eval_set,  verbose=False)\n",
    "y2_pred = model.predict(X_train2)\n",
    "predictions = [round(value) for value in y2_pred]\n",
    "accuracy = accuracy_score(y_train2, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(X_train1.columns)\n",
    "feature_importance = model.feature_importances_\n",
    "features_in_model = all_features\n",
    "features_in_model = [all_features[i] for i in range(len(feature_importance)) if feature_importance[i] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var22-APROVA</td>\n",
       "      <td>0.512428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rate_low_freq_app</td>\n",
       "      <td>0.053278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>var10-Autonomos</td>\n",
       "      <td>0.041230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>var2-O</td>\n",
       "      <td>0.031363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>WhatsApp</td>\n",
       "      <td>0.030622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Empréstimo Pessoal</td>\n",
       "      <td>0.026959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>var21-FALHA</td>\n",
       "      <td>0.023638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>var18</td>\n",
       "      <td>0.022959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>var17</td>\n",
       "      <td>0.021836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>var31-NAO_CONSTAM_OCORRENCIAS</td>\n",
       "      <td>0.021113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         var_name  importance\n",
       "11                   var22-APROVA    0.512428\n",
       "20              rate_low_freq_app    0.053278\n",
       "7                 var10-Autonomos    0.041230\n",
       "4                          var2-O    0.031363\n",
       "58                       WhatsApp    0.030622\n",
       "31             Empréstimo Pessoal    0.026959\n",
       "9                     var21-FALHA    0.023638\n",
       "1                           var18    0.022959\n",
       "0                           var17    0.021836\n",
       "13  var31-NAO_CONSTAM_OCORRENCIAS    0.021113"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建新的dataframe存储变量的重要性\n",
    "var_importance = pd.DataFrame(columns=[\"var_name\", 'importance'])\n",
    "var_importance['var_name'] = all_features\n",
    "var_importance['importance'] = feature_importance\n",
    "var_importance.sort_values('importance',  ascending=False).to_csv(file_path_model + 'var_importance.csv')\n",
    "var_importance.sort_values('importance',  ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>数据源</th>\n",
       "      <th>指标英文</th>\n",
       "      <th>指标中文</th>\n",
       "      <th>数据类型</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bank</td>\n",
       "      <td>var17</td>\n",
       "      <td>var17</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bank</td>\n",
       "      <td>var18</td>\n",
       "      <td>var18</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bank</td>\n",
       "      <td>var9</td>\n",
       "      <td>var9</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bank</td>\n",
       "      <td>age</td>\n",
       "      <td>age</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bank</td>\n",
       "      <td>var2-O</td>\n",
       "      <td>var2-O</td>\n",
       "      <td>uint8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>othersdk</td>\n",
       "      <td>electric_quantity</td>\n",
       "      <td>electric_quantity</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>othersdk</td>\n",
       "      <td>available_memory_size</td>\n",
       "      <td>available_memory_size</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>othersdk</td>\n",
       "      <td>total_memory_size</td>\n",
       "      <td>total_memory_size</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>othersdk</td>\n",
       "      <td>available_capacity_size</td>\n",
       "      <td>available_capacity_size</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>othersdk</td>\n",
       "      <td>disk_size</td>\n",
       "      <td>disk_size</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         数据源                     指标英文                     指标中文     数据类型\n",
       "0       bank                    var17                    var17  float64\n",
       "1       bank                    var18                    var18  float64\n",
       "2       bank                     var9                     var9  float64\n",
       "3       bank                      age                      age  float64\n",
       "4       bank                   var2-O                   var2-O    uint8\n",
       "..       ...                      ...                      ...      ...\n",
       "68  othersdk        electric_quantity        electric_quantity  float64\n",
       "69  othersdk    available_memory_size    available_memory_size  float64\n",
       "70  othersdk        total_memory_size        total_memory_size  float64\n",
       "71  othersdk  available_capacity_size  available_capacity_size  float64\n",
       "72  othersdk                disk_size                disk_size  float64\n",
       "\n",
       "[73 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_dict = pd.read_csv(file_path_model + 'var_dict.csv')\n",
    "var_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 计算PSI  IV\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yantingting/anaconda3/envs/modeling/lib/python3.7/site-packages/pandas/core/series.py:856: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/yantingting/anaconda3/envs/modeling/lib/python3.7/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n",
      "/Users/yantingting/anaconda3/envs/modeling/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 计算PSI  IV\n",
    "'''\n",
    "time1 = time.time()\n",
    "args_dict = {\n",
    "    'random_forest': {\n",
    "    'grid_search': False,  # 选择了True则会进行网格筛选速度会比较慢\n",
    "    'param': None\n",
    "    },\n",
    "    'xgboost': {\n",
    "    'grid_search': False,\n",
    "    'param': None\n",
    "    }\n",
    "}\n",
    "methods = [\n",
    "    'random_forest',\n",
    "    # 'lasso',\n",
    "    'xgboost'\n",
    "]\n",
    "\n",
    "fs_obj = fs.FeatureSelection()\n",
    "bin_obj = metrics.BinWoe()\n",
    "model_data_final = df_train1.copy()\n",
    "features_var_dict = list(var_dict['指标英文'])\n",
    "X_IV = model_data_final[features_var_dict]\n",
    "y_IV = model_data_final['flag_result'].astype(int)\n",
    "X_cat_train, X_transformed, woe_iv_df, rebin_spec, ranking_result = fs_obj.overall_ranking(X_IV, y_IV, var_dict, args_dict, methods, num_max_bins=5)\n",
    "rebin_spec_bin_adjusted = {k: v for k, v in rebin_spec.items() if k in features_var_dict}\n",
    "# 输出的是变量的分箱\n",
    "X_cat_train = bin_obj.convert_to_category( model_data_final[features_var_dict],  var_dict, rebin_spec_bin_adjusted)\n",
    "\n",
    "model_data_final['appmon'] = model_data_final['DATE'].apply(lambda x: 1 if x > 20191001 else 0)\n",
    "\n",
    "X_cat_train_with_y_appmon = pd.merge(X_cat_train, model_data_final[['flag_result', 'appmon']], left_index=True,\n",
    "                                     right_index=True)\n",
    "var_dist_badRate_by_time = ss.get_badRate_and_dist_by_time(X_cat_train_with_y_appmon, features_var_dict, 'appmon',   'flag_result')\n",
    "var_dist_badRate_by_time.columns = [\n",
    "    'varName',\n",
    "    'bins',\n",
    "    'total1',\n",
    "    'tolal2',\n",
    "    'bad1',\n",
    "    'bad2',\n",
    "    'dist1',\n",
    "    'dist2',\n",
    "    'badrate1',\n",
    "    'badrate2']\n",
    "var_dist_badRate_by_time.to_csv(file_path_model + \"var_dist_badRate_by_time.csv\", index=False, sep=',')\n",
    "time2 = time.time()\n",
    "print('run_time : ', time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dist_badRate_by_time_ = pd.read_csv(file_path_model + \"var_dist_badRate_by_time.csv\")\n",
    "# CALCULATE PSI\n",
    "var_dist_badRate_by_time_['PSI'] = (var_dist_badRate_by_time_['dist1'] - var_dist_badRate_by_time_['dist2']) * np.log(\n",
    "    var_dist_badRate_by_time_['dist1'] / var_dist_badRate_by_time_['dist2'])\n",
    "\n",
    "var_psi = pd.DataFrame(var_dist_badRate_by_time_.groupby(['varName'])['PSI'].agg(['sum']))\n",
    "var_psi['varName'] = var_psi.index\n",
    "var_psi.columns = ['PSI', 'varName']\n",
    "var_psi = var_psi[[ 'varName' ,  'PSI']]\n",
    "features_stable = list(var_psi[var_psi['PSI'] <= 0.2]['varName'])\n",
    "features_unstable = list(var_psi[var_psi['PSI'] > 0.2]['varName'])\n",
    "features_iv_hi = list(ranking_result[ranking_result['IV'] >= 0.02]['指标英文'])\n",
    "features_iv_lo = list(ranking_result[ranking_result['IV'] < 0.02]['指标英文'])\n",
    "var_psi.to_csv(file_path_model + \"var_psi.csv\", index=False, sep=',')\n",
    "woe_iv_df.to_csv(file_path_model + \"woe_iv_df_.csv\", index=False, sep=',')\n",
    "features_remove = set(features_unstable + features_iv_lo)\n",
    "features_in_model = [ele for ele in features_in_model if ele not in features_remove]\n",
    "features =pd.DataFrame({\"var\": features_in_model})\n",
    "features['数据源'] = features['var'].map(lambda x: dict(zip(var_dict['指标英文'], var_dict['数据源']))[x])\n",
    "features = features[features['数据源'] != 'bank']\n",
    "features_in_model = features['var'].to_list()\n",
    "features.to_csv(file_path_model + \"features_in_model.csv\")\n",
    "features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------------\n",
    "XGB tuning parameters\n",
    "-------------------------------\n",
    "'''\n",
    "'''\n",
    "参数调优的一般步骤\n",
    "1. 确定学习速率和提升参数调优的初始值\n",
    "2. max_depth 和 min_child_weight 参数调优\n",
    "3. gamma参数调优\n",
    "4. subsample 和 colsample_bytree 参数优\n",
    "5. 正则化参数alpha调优\n",
    "6. 降低学习速率和使用更多的决策树\n",
    "'''\n",
    "\n",
    "df_train1_learn = df_train1[features_in_model]\n",
    "df_train2_learn = df_train2[features_in_model]\n",
    "X_train1 = df_train1_learn.values\n",
    "X_train2 = df_train2_learn.values\n",
    "\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth': range(3, 10, 1),\n",
    "    'min_child_weight': range(1, 6, 1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, n_estimators=90, gamma=0, subsample=0.7,\n",
    "                                                max_depth=4, colsample_bytree=1, objective='binary:logistic',\n",
    "                                                nthread=6, scale_pos_weight=1, seed=27),\n",
    "                        param_grid=param_test1, scoring='roc_auc', n_jobs=4, iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(X_train1, y_train1)\n",
    "\n",
    "best_max_depth = gsearch1.best_params_['max_depth']\n",
    "best_min_child_weight = gsearch1.best_params_['min_child_weight']\n",
    "print('1:', gsearch1.best_score_)\n",
    "\n",
    "param_test2 = {'gamma': [i / 10.0 for i in range(0, 5)]}\n",
    "gsearch2 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, n_estimators=90, subsample=0.7, colsample_bytree=1,\n",
    "                                                max_depth=best_max_depth,\n",
    "                                                min_child_weight=best_min_child_weight, objective='binary:logistic',\n",
    "                                                nthread=6, scale_pos_weight=1, seed=27),\n",
    "                        param_grid=param_test2, scoring='roc_auc', n_jobs=4, iid=False, cv=5)\n",
    "gsearch2.fit(X_train1, y_train1)\n",
    "best_gamma = gsearch2.best_params_['gamma']\n",
    "print('2:', gsearch2.best_score_)\n",
    "\n",
    "\n",
    "param_test3 = {\n",
    "    'subsample': [i / 10.0 for i in range(6, 10)],\n",
    "    'colsample_bytree': [i / 10.0 for i in range(6, 10)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, n_estimators=90, max_depth=best_max_depth, gamma=best_gamma,\n",
    "                                                min_child_weight=best_min_child_weight, objective='binary:logistic', nthread=6,\n",
    "                                                scale_pos_weight=1, seed=27),\n",
    "                        param_grid=param_test3, scoring='roc_auc', n_jobs=4, iid=False, cv=5)\n",
    "gsearch3.fit(X_train1, y_train1)\n",
    "best_subsample = gsearch3.best_params_['subsample']\n",
    "best_colsample_bytree = gsearch3.best_params_['colsample_bytree']\n",
    "print('3:', gsearch3.best_score_)\n",
    "\n",
    "\n",
    "param_test4 = {'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]}\n",
    "gsearch4 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, n_estimators=90, max_depth=best_max_depth, gamma=best_gamma,\n",
    "                                                colsample_bytree=best_colsample_bytree, subsample=best_subsample,\n",
    "                                                min_child_weight=best_min_child_weight, objective='binary:logistic', nthread=6,\n",
    "                                                scale_pos_weight=1, seed=27),\n",
    "                        param_grid=param_test4, scoring='roc_auc', n_jobs=4, iid=False, cv=5)\n",
    "gsearch4.fit(X_train1, y_train1)\n",
    "best_reg_alpha = gsearch4.best_params_['reg_alpha']\n",
    "print('4:', gsearch4.best_score_)\n",
    "\n",
    "\n",
    "param_test5 = {'n_estimators': range(50, 500, 20)}\n",
    "gsearch5 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, max_depth=best_max_depth, gamma=best_gamma,\n",
    "                                                colsample_bytree=best_colsample_bytree, subsample=best_subsample,\n",
    "                                                reg_alpha=best_reg_alpha,\n",
    "                                                min_child_weight=best_min_child_weight, objective='binary:logistic',\n",
    "                                                nthread=6, scale_pos_weight=1, seed=27),\n",
    "                        param_grid=param_test5, scoring='roc_auc', n_jobs=4, iid=False, cv=5)\n",
    "gsearch5.fit(X_train1, y_train1)\n",
    "best_n_estimators = gsearch5.best_params_['n_estimators']\n",
    "print('5:', gsearch5.best_score_)\n",
    "\n",
    "\n",
    "param_test6 = {'learning_rate': [i / 100.0 for i in range(0, 30)]}\n",
    "gsearch6 = GridSearchCV(estimator=XGBClassifier(max_depth=best_max_depth, gamma=best_gamma, n_estimators=best_n_estimators,\n",
    "                                                colsample_bytree=best_colsample_bytree, subsample=best_subsample, reg_alpha=best_reg_alpha,\n",
    "                                                min_child_weight=best_min_child_weight, objective='binary:logistic', nthread=6,\n",
    "                                                scale_pos_weight=1, seed=27),\n",
    "                        param_grid=param_test6, scoring='roc_auc', n_jobs=4, iid=False, cv=5)\n",
    "gsearch6.fit(X_train1, y_train1)\n",
    "best_learning_rate = gsearch6.best_params_['learning_rate']\n",
    "print('6:', gsearch6.best_score_)\n",
    "\n",
    "# 用获取得到的最优参数再次训练模型\n",
    "best_xgb = XGBClassifier(learning_rate=best_learning_rate, n_estimators=best_n_estimators, max_depth=best_max_depth,\n",
    "                         gamma=best_gamma, colsample_bytree=best_colsample_bytree, subsample=best_subsample, reg_alpha=best_reg_alpha,\n",
    "                         min_child_weight=best_min_child_weight,\n",
    "                         objective='binary:logistic', nthread=6, scale_pos_weight=1, eval_metric='auc', seed=27)\n",
    "print(best_xgb)\n",
    "best_xgb.fit(X_train1, y_train1)\n",
    "print('process is end!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob2Score(prob, basePoint, PDO):\n",
    "    y = np.log(prob / (1 - prob))\n",
    "    return (basePoint + PDO / np.log(2) * (-y)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------------\n",
    "Model scoring\n",
    "-------------------------------\n",
    "'''\n",
    "df_all_learn = df_var[features_in_model]\n",
    "X_df_all = df_all_learn.values\n",
    "y_df_all = df_var['flag_result'].astype(int)\n",
    "y_df_all_pred = best_xgb.predict_proba(X_df_all)[:, 1]\n",
    "\n",
    "score_all = [round(Prob2Score(value, 600, 20)) for value in y_df_all_pred]\n",
    "predictions = [round(value) for value in y_df_all_pred]\n",
    "mylist = ['loan_id', 'flag_result', 'DATE']\n",
    "# SettingWithCopyWarning出现的原因  链式赋值/Chained Assignment  所以在后边加一个copy()\n",
    "df_all_result = df_var[mylist].copy()\n",
    "df_all_result.loc[:, 'predictions'] = predictions\n",
    "df_all_result.loc[:, 'y_df_pred'] = y_df_all_pred\n",
    "df_all_result.loc[:, 'score_all'] = score_all\n",
    "df_all_result.loc[:, 'group'] = pd.qcut(pd.DataFrame(\n",
    "    score_all)[0], q=10, duplicates='drop', precision=0).astype(str)\n",
    "df_all_result.to_csv(file_path_model + \"df_all_result.csv\")\n",
    "accuracy = accuracy_score(y_df_all, predictions)\n",
    "print(\"AccuracyAll: %.2f%%\" % (accuracy * 100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "-------------------------------\n",
    "Model evaluation\n",
    "-------------------------------\n",
    "'''\n",
    "\n",
    "df_train1_result = pd.merge(df_train1[['loan_id']], df_all_result, on='loan_id', how='left' )\n",
    "df_train2_result = pd.merge(df_train2[['loan_id']], df_all_result, on='loan_id', how='left' )\n",
    "df_test_result = pd.merge(df_test[['loan_id']], df_all_result, on='loan_id', how='left' )\n",
    "\n",
    "# AUC+Accuracy\n",
    "train1_auc = roc_auc_score(df_train1_result['flag_result'], df_train1_result['y_df_pred'])\n",
    "print(\"train1_auc: %.2f\" % train1_auc)\n",
    "train2_auc = roc_auc_score(df_train2_result['flag_result'], df_train2_result['y_df_pred'])\n",
    "print(\"train2_auc: %.2f\" % train1_auc)\n",
    "test_auc = roc_auc_score(df_test_result['flag_result'], df_test_result['y_df_pred'])\n",
    "print(\"test_auc: %.2f\" % test_auc)\n",
    "train1_accuracy = accuracy_score(\n",
    "    df_train1_result['flag_result'],\n",
    "    df_train1_result['predictions'])\n",
    "print(\"train1_accuracy: %.2f%%\" % (train1_accuracy * 100.0))\n",
    "train2_accuracy = accuracy_score(\n",
    "    df_train2_result['flag_result'],\n",
    "    df_train2_result['predictions'])\n",
    "print(\"train2_accuracy: %.2f%%\" % (train2_accuracy * 100.0))\n",
    "test_accuracy = accuracy_score(\n",
    "    df_test_result['flag_result'],\n",
    "    df_test_result['predictions'])\n",
    "print(\"test_accuracy: %.2f%%\" % (test_accuracy * 100.0))\n",
    "\n",
    "# KS\n",
    "def ModelBin(data):\n",
    "    bad = data['flag_result'].sum()\n",
    "    good = data['flag_result'].count() - bad\n",
    "    df = data.groupby(data['group'])\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['total'] = df[\"flag_result\"].count()\n",
    "    df1['bad'] = df[\"flag_result\"].sum()\n",
    "    df1['good'] = df1['total'] - df1[\"bad\"]\n",
    "    df1['per%'] = df1['total'] / data['flag_result'].count()\n",
    "    df1['overdue'] = df1['bad'] / df1['total']\n",
    "    df1['cum_bad'] = df1['bad'].cumsum(0) / bad\n",
    "    df1['cum_good'] = df1['good'].cumsum(0) / good\n",
    "    df1[\"KS\"] = df1['cum_bad'] - df1['cum_good']\n",
    "    df1[\"cum_overdue\"] = df1[\"bad\"].cumsum(0) / df1['total'].cumsum(0)\n",
    "\n",
    "    df1.columns = [\"total\", \"bad\", \"good\", \"per%\", \"overdue\",\n",
    "                   \"cum_bad\", \"cum_good\", \"KS\", \"cum_overdue\"]\n",
    "    return df1\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(file_path_model + \"result.xlsx\")\n",
    "ModelBin(df_train1_result).to_excel(writer, sheet_name=\"train1\")\n",
    "ModelBin(df_train2_result).to_excel(writer, sheet_name=\"train2\")\n",
    "ModelBin(df_test_result).to_excel(writer, sheet_name=\"test\")\n",
    "writer.save()\n",
    "\n",
    "'''\n",
    "-------------------------------\n",
    "SAVE MODEL\n",
    "-------------------------------\n",
    "'''\n",
    "best_xgb.save_model(file_path_model + 'bx_sdk_model.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "入模变量的分布(uniVarChart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.read_csv(file_path_model + \"features_in_model.csv\")\n",
    "feature.shape\n",
    "feature_list = feature['var'].tolist()\n",
    "time1 = time.time()\n",
    "df_train1_train2 = df_var[(df_var['is_sample'] == 1)|(df_var['is_sample'] == 2)]\n",
    "df_train1 = df_var[df_var['is_sample'] == 1]\n",
    "df_train2 = df_var[df_var['is_sample'] == 2]\n",
    "wrong_list= []\n",
    "for col in feature_list:\n",
    "    print('{} is in processing'.format(col))\n",
    "    try:\n",
    "        pl.uniVarChart(df_train1_train2.fillna(-1),  col, target = 'flag_result', n_bins = 10, result_path=file_path_model ,dfltValue = -99999, dftrain =df_train1.fillna(-1), dftest = df_train2.fillna(-1), \n",
    "                drawAll = True, drawTrTe = True)\n",
    "    except IndexError:\n",
    "        wrong_list.append(col)\n",
    "        print('wrong!')\n",
    "print(wrong_list)\n",
    "time2 = time.time()\n",
    "print('run_time: ', time2-time1)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mymodel = xgb.Booster()\n",
    "# model1 = mymodel.load_model(file_path_model + 'bx_sdk_model.model')\n",
    "\n",
    "sys.path.append('/Users/yantingting/Documents/newgenie/newgenie')\n",
    "var_importance= pd.read_csv(file_path_model + 'var_importance.csv', usecols=[1,2])\n",
    "feature = pd.read_csv(file_path_model + \"features_in_model.csv\")\n",
    "model_importance = pd.merge(feature[['var']], var_importance, left_on = 'var', right_on = 'var_name', how='left')\n",
    "model_importance = model_importance.drop('var_name', axis = 1)\n",
    "# model_importance.head()\n",
    "test_pred = pd.concat([df_train1.iloc[:, 4:], df_train2.iloc[:, 4:]], axis =0)\n",
    "model = xgb.Booster()\n",
    "model.load_model(file_path_model + 'bx_sdk_model.model')\n",
    "model_importance.head()\n",
    "model_importance.columns = ['varName', 'importance']                       \n",
    "import utils3.generate_report as gr\n",
    "f_imp_list = gr.get_feature_importance(model_importance)\n",
    "import utils3.plot_tools as pt\n",
    "import matplotlib.pyplot as plt\n",
    "RESULT_PATH = file_path_model\n",
    "FIG_PATH = os.path.join(RESULT_PATH, 'figure', 'PDP')\n",
    "if not os.path.exists(FIG_PATH):\n",
    "    os.makedirs(FIG_PATH)\n",
    "\n",
    "n=0\n",
    "while n <len(f_imp_list):\n",
    "    m = n+9\n",
    "    features_draw=[i for i in f_imp_list[n:m]]\n",
    "    select_features = model_importance['varName'].values.tolist()\n",
    "    pt.pdpCharts9(model, test_pred, features_draw, select_features, n_bins=10, dfltValue = -1)\n",
    "    path = os.path.join(FIG_PATH,\"pdp_\"+str(n)+\"_\"+str(m)+\".png\")\n",
    "    plt.savefig(path, format='png', dpi=100)\n",
    "    plt.close()\n",
    "    n += 9\n",
    "    \n",
    "print('end!')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "liftchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib.pylab import rcParams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from matplotlib.pylab import rcParams\n",
    "from sklearn.externals import joblib\n",
    "import platform\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "rcParams['figure.figsize'] = 16,8\n",
    "\n",
    "def calc_rate(df, y_true):\n",
    "    return pd.DataFrame.from_dict(\n",
    "        {\n",
    "            'cntLoan': len(df),\n",
    "            'event': df[y_true].sum(),\n",
    "            'eventRate': df[y_true].mean()\n",
    "        },\n",
    "        orient='index').T\n",
    "\n",
    "\n",
    "def show_result_new(df, y_pred, y_true, n_bins, feature_label='', result_path = None):\n",
    "    '''\n",
    "    模型预测结果的Lift Chart.\n",
    "\n",
    "    :param df: 数据集, DataFrame.\n",
    "    :param y_pred: 模型预测分数, str.\n",
    "    :param y_true: 是否违约标签列, array[0, 1].\n",
    "    :param n_bins: 分箱数量, int.\n",
    "    :param feature_label:\n",
    "    :return: Lift Chart, fone\n",
    "    '''\n",
    "\n",
    "    if feature_label == '':\n",
    "        feature_label = y_pred\n",
    "    df['bkl_%s' % y_pred] = pd.qcut(df[y_pred], n_bins, duplicates='drop')\n",
    "    n_bins = len(df['bkl_%s' % y_pred].unique())\n",
    "    print('分箱数量: ', n_bins)\n",
    "\n",
    "    g_df = df.groupby('bkl_%s' % y_pred).apply(lambda x: calc_rate(x, y_true)).reset_index(level=1, drop=True)\n",
    "    g_df['acmLoan'] = g_df['cntLoan'].cumsum()\n",
    "    g_df['acmEvent'] = g_df['event'].cumsum()\n",
    "    g_df['acmEventRate'] = g_df['acmEvent'] / g_df['acmLoan']\n",
    "    g_df = g_df.reset_index()\n",
    "    #print(g_df)\n",
    "    # plot lift_chart - marginal\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    g_df.index = range(1, n_bins + 1)\n",
    "    plt.plot(g_df.index, g_df['eventRate'], marker='o',\n",
    "             label='Auc of %s:%d:%.3f' % (\n",
    "                 feature_label, df.shape[0], np.round(metrics.roc_auc_score(df[y_true], df[y_pred]), 3)))  # linestyle='--'\n",
    "    plt.title('EventRate in %d Quantiles' % n_bins)\n",
    "    plt.ylabel('eventRate')\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=13, loc=2, framealpha=0.5)\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(g_df.index, g_df['acmEventRate'], marker='o',\n",
    "             label='Auc of %s:%d:%.3f' % (\n",
    "                 feature_label, df.shape[0], np.round(metrics.roc_auc_score(df[y_true], df[y_pred]), 3)))  # linestyle='--'\n",
    "    plt.title('Accum-EventRate in %d Quantiles' % n_bins)\n",
    "    plt.ylabel('accumEventRate')\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=13, loc=2, framealpha=0.5)\n",
    "    rcParams['figure.figsize'] = 16, 8\n",
    "    plt.tight_layout()\n",
    "    if result_path is not None:\n",
    "        result_path = os.path.join(result_path, 'figure/liftChart/')\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    plt.savefig(os.path.join(result_path, 'lift_chart_overall.png'), format='png', dpi=80)\n",
    "    plt.show()\n",
    "    return g_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/yantingting/Documents/newgenie/newgenie')\n",
    "import plot_tools as pt\n",
    "import matplotlib.pyplot as plt\n",
    "df_all_result = pd.read_csv(file_path_model + \"df_all_result.csv\")\n",
    "df_train1_result = pd.merge(df_train1[['loan_id']], df_all_result, on='loan_id', how='left' )\n",
    "df_train2_result = pd.merge(df_train2[['loan_id']], df_all_result, on='loan_id', how='left' )\n",
    "df_test_result = pd.merge(df_test[['loan_id']], df_all_result, on='loan_id', how='left' )\n",
    "train_pred = df_train1_result\n",
    "test_pred = df_train2_result\n",
    "testnew_pred= df_test_result\n",
    "result_path = file_path_model\n",
    "train_lc = show_result_new(train_pred, 'y_df_pred','flag_result', n_bins = 10, result_path = result_path, feature_label='train')\n",
    "test_lc = show_result_new(test_pred, 'y_df_pred','flag_result', n_bins = 10, result_path = result_path, feature_label='test')\n",
    "oot_lc = show_result_new(testnew_pred, 'y_df_pred','flag_result', n_bins = 10, result_path = result_path, feature_label='OOT')\n",
    "plt.show()\n",
    "plt.savefig('lift_chart_overall.png', dpi=100)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
